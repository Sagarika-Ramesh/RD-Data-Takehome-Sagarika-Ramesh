The code(src.ipynb) is present in the folder question_01.

4. Now, consider the case where you had to manage a dataset with millions images rather than a few hundred. 
How will you change your dataset building and storing methods for:

a. Faster access, given that data data lives on the cloud infrastructure like S3 
Answer: 
AWS Sage Maker and S3 buckets are used to train and deploy the models quickly. 
The AWS Sagemaker already has few pretrained models for deployment.
It is quicker and easy to train and deploy models using AWS Sagemaker.
To ensure faster access, I will take the below actions:
> Will impliment parallel processing to access images quicker. 
This can be done using python's multi threading and multiprocessing libraries 
>Can automate the ETL(Extraction Transform Load) process. 
>Will access data in batch and store them in buffer or cache.

b. Faster data re-sampling, to create custom datasets
Answer: We can use Spark to resample data in parallel. Apache Kafk can be used for streaming data processing.

c. Faster data-loader access for faster training
Answer: Data pipeline is most efficient for this. We can preload the data using data loader.
