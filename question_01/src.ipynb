{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7512c56e",
   "metadata": {},
   "source": [
    "______________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "37c54761",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "#from keras_vggface.vggface import VGGFace\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c689e65c",
   "metadata": {},
   "source": [
    "### DATASET CREATION\n",
    "\n",
    "The below code is written to organize a dataset of face images into training and validation sets. My goal is to split the dataset with an 80/20 ratio.\n",
    "I've defined source and destination directories for my dataset.\n",
    "To ensure randomness in the split, I've shuffled the list of files.\n",
    "Then, I've separated the shuffled list of files into two sets: one for training and one for validation, based on the calculated split index.\n",
    "I've copied the files from the source directory to their respective training and validation directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6465c07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 1081, Training files: 864, Validation files: 217\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "source_dir = '../dataset/face_dataset/face_real'  \n",
    "train_dir = '../dataset/face_dataset/train/face_real'  \n",
    "val_dir = '../dataset/face_dataset/validation/face_real'\n",
    "\n",
    "# Define the split ratio (e.g., 80% for training, 20% for validation)\n",
    "split_ratio = 0.8\n",
    "\n",
    "# Create destination directories if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# List all files in the source directory (assuming all files are images)\n",
    "files = os.listdir(source_dir)\n",
    "\n",
    "# Shuffle the list of files randomly\n",
    "random.shuffle(files)\n",
    "\n",
    "# Calculate the split index\n",
    "split_index = int(len(files) * split_ratio)\n",
    "\n",
    "# Split the files into training and validation sets\n",
    "train_files = files[:split_index]\n",
    "val_files = files[split_index:]\n",
    "\n",
    "# Copy the files to the respective directories\n",
    "for file in train_files:\n",
    "    src = os.path.join(source_dir, file)\n",
    "    dst = os.path.join(train_dir, file)\n",
    "    copyfile(src, dst)\n",
    "\n",
    "for file in val_files:\n",
    "    src = os.path.join(source_dir, file)\n",
    "    dst = os.path.join(val_dir, file)\n",
    "    copyfile(src, dst)\n",
    "\n",
    "print(f\"Total files: {len(files)}, Training files: {len(train_files)}, Validation files: {len(val_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ecec0233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 960, Training files: 768, Validation files: 192\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "source_dir = '../dataset/face_dataset/face_fake'  \n",
    "train_dir = '../dataset/face_dataset/train/face_fake'  \n",
    "val_dir = '../dataset/face_dataset/validation/face_fake'\n",
    "\n",
    "# Define the split ratio (e.g., 80% for training, 20% for validation)\n",
    "split_ratio = 0.8\n",
    "\n",
    "# Create destination directories if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# List all files in the source directory (assuming all files are images)\n",
    "files = os.listdir(source_dir)\n",
    "\n",
    "# Shuffle the list of files randomly\n",
    "random.shuffle(files)\n",
    "\n",
    "# Calculate the split index\n",
    "split_index = int(len(files) * split_ratio)\n",
    "\n",
    "# Split the files into training and validation sets\n",
    "train_files = files[:split_index]\n",
    "val_files = files[split_index:]\n",
    "\n",
    "# Copy the files to the respective directories\n",
    "for file in train_files:\n",
    "    src = os.path.join(source_dir, file)\n",
    "    dst = os.path.join(train_dir, file)\n",
    "    copyfile(src, dst)\n",
    "\n",
    "for file in val_files:\n",
    "    src = os.path.join(source_dir, file)\n",
    "    dst = os.path.join(val_dir, file)\n",
    "    copyfile(src, dst)\n",
    "\n",
    "print(f\"Total files: {len(files)}, Training files: {len(train_files)}, Validation files: {len(val_files)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771c82c4",
   "metadata": {},
   "source": [
    "I've configured an ImageDataGenerator with the following settings:\n",
    "\n",
    "I have performed Data augmentation by enabled horizontal flipping fo images.\n",
    "I have performed image preprocessing by rescaled the pixel values of the images to a range between 0 and 1 (`rescale=1./255`).\n",
    "\n",
    "Next, I've created two data generators for training and validation data:\n",
    "\n",
    "- The `train` data generator is configured to flow from the directory \"../dataset/face_dataset/train/\". It's set to work in binary classification mode (`class_mode=\"binary\"`) and resize images to a target size of 224x224 pixels. Each batch will contain 64 images.\n",
    "- Similarly, the `val` data generator flows from the directory \"../dataset/face_dataset/validation/\", operates in binary classification mode, resizes images to 224x224 pixels, and uses batches of 64 images.\n",
    "\n",
    "These data generators will be used to load and preprocess the training and validation datasets for feeding into a deep learning model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0884debb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1632 images belonging to 2 classes.\n",
      "Found 409 images belonging to 2 classes.\n",
      "Found 200 images belonging to 2 classes.\n",
      "Found 22 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "data_with_aug = ImageDataGenerator(horizontal_flip=True,\n",
    "                                   vertical_flip=False,\n",
    "                                   rescale=1./255)\n",
    "train = data_with_aug.flow_from_directory(\"../dataset/face_dataset/train/\",\n",
    "                                          class_mode=\"binary\",\n",
    "                                          target_size=(224, 224),\n",
    "                                          batch_size=64)\n",
    "\n",
    "val = data_with_aug.flow_from_directory(\"../dataset/face_dataset/validation/\",\n",
    "                                          class_mode=\"binary\",\n",
    "                                          target_size=(224, 224),\n",
    "                                          batch_size=64)\n",
    "test = data_with_aug.flow_from_directory(\"../dataset/face_dataset/test/\",\n",
    "                                          class_mode=\"binary\",\n",
    "                                          target_size=(224, 224),\n",
    "                                          batch_size=64)\n",
    "real_test = data_with_aug.flow_from_directory(\"../rd_test_dataset/\",\n",
    "                                          class_mode=\"binary\",\n",
    "                                          target_size=(224, 224),\n",
    "                                          batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "04f299b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'face_fake': 0, 'face_real': 1}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2cfcc5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5565fde",
   "metadata": {},
   "source": [
    "### Deep Classifier used for the classification:\n",
    "#### 1)VGG model - transfer learning(for feature extraction)\n",
    "#### 2)CNN model\n",
    "#### 3)VGGFace - transfer learning(for feature extraction)\n",
    "#### All these models are train on fewer epochs and small dataset. The performance of these models can be increased with fine tuning the model (by adding more layers, Hyperparameter Tuning..etc )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b442ec6",
   "metadata": {},
   "source": [
    "I've harnessed the power of transfer learning to achieve higher performance in my project. For this purpose, I opted for a VGG model, notably VGG-16, which, during its development, had outperformed the competition in the ImageNet photo classification challenge.\n",
    "\n",
    "This model consists of two fundamental components: the feature extraction part, composed of VGG blocks that excel at capturing image details, and the classifier part, consisting of fully connected layers culminating in an output layer.\n",
    "\n",
    "To adapt this model to my task of discerning real from fake faces, I strategically leveraged the feature extraction segment. Here's the key strategy:\n",
    "\n",
    "- I maintained the convolutional layers' weights unaltered during training, benefiting from their ability to detect meaningful features in images.\n",
    "- I introduced a classifier segment tailored specifically to the real and fake face dataset. This classifier, composed of fully connected layers, it is designed to interpret the features extracted by the pre-trained layers and perform a binary classification, efficiently categorizing the images as either real or fake.\n",
    "\n",
    "This approach harnesses the strengths of transfer learning, making use of a pre-trained feature extractor while fine-tuning the model to excel in distinguishing between genuine and manipulated facial images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3b11f70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model():\n",
    "\n",
    " model = VGG16(include_top=False, input_shape=(224, 224, 3))\n",
    " for layer in model.layers:\n",
    "     layer.trainable = False\n",
    "     flat1 = Flatten()(model.layers[-1].output)\n",
    "     class1 = Dense(2048, activation='relu')(flat1)\n",
    "     output = Dense(1, activation='sigmoid')(class1)#output layer with sigmoid activation function\n",
    " model = Model(inputs=model.inputs, outputs=output)\n",
    " \n",
    " model.compile(loss='binary_crossentropy',optimizer=Adam(0.0002), metrics=['acc'])\n",
    " return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd84297",
   "metadata": {},
   "source": [
    "The below code is used to create and save a plot summarizing a deep learning model's training and validation performance. It plots both the loss and accuracy over training epochs and saves the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e6cbc62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_diagnostics(history):\n",
    " # plot loss\n",
    " plt.subplot(211)\n",
    " plt.title('Cross Entropy Loss')\n",
    " plt.plot(history.history['loss'], color='blue', label='train')\n",
    " plt.plot(history.history['val_loss'], color='orange', label='val')\n",
    " # plot accuracy\n",
    " plt.subplot(212)\n",
    " plt.title('Classification Accuracy')\n",
    " plt.plot(history.history['accuracy'], color='blue', label='train')\n",
    " plt.plot(history.history['val_accuracy'], color='orange', label='val')\n",
    " # save plot to file\n",
    " filename = sys.argv[0].split('/')[-1]\n",
    " plt.savefig(filename + '_plot.png')\n",
    " plt.close()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe09736",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60466125",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train, steps_per_epoch=len(train),\n",
    "validation_data=val, validation_steps=len(val), epochs=30, verbose=1)\n",
    " # save model\n",
    "model.save('final_model.h5')\n",
    "_, acc = model.evaluate_generator(val, steps=len(val), verbose=0)\n",
    "print('> %.3f' % (acc * 100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ab7684",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.title('Cross Entropy Loss')\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='orange', label='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd510f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(212)\n",
    "plt.title('Classification Accuracy')\n",
    "plt.plot(history.history['acc'], color='blue', label='train')\n",
    "plt.plot(history.history['val_acc'], color='orange', label='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "f4a66ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = sys.argv[0].split('/')[-1]\n",
    "plt.savefig(filename + '_plot.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ff7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(real_test)\n",
    "y_test = real_test.classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9070568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"ROC-AUC Score:\", metrics.roc_auc_score(y_test, y_pred))\n",
    "\n",
    "threshold = 0.5\n",
    "pred = (y_pred > threshold).astype(int)\n",
    "\n",
    "accuracy = (pred == y_test).mean()\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5b1459",
   "metadata": {},
   "source": [
    "1. Design & build a small dataset (about 100+ images) to differentiate between real and fake face images. Please explain:\n",
    "   \n",
    " \n",
    "a. Considerations that went into deciding what data to collect.\n",
    "Answer: \n",
    "I have tried to build the dataset using 2 different methods:\n",
    "Method1:\n",
    "Data for Train and Validation:\n",
    "Real images are obtained from Flickr. (Flickr-Faces-HQ (FFHQ)). 1082 random images are selected.\n",
    "Fake images are obtained from A Style-Based Generator Architecture for Generative Adversarial Networks created by\n",
    "Tero Karras (NVIDIA), Samuli Laine (NVIDIA), Timo Aila (NVIDIA). It is made publicly available for research purposes.\n",
    "961 random images are selected. The data set is not imbalanced. There is no bias as the dataset contains people from different regions, race, age, gender and pose.\n",
    "\n",
    "Data for Test(Unseen Data):\n",
    "For test set, I have collected data from seperate source to avoid data leakage. Data is obtained from Computational Intelligence and Photography Lab,Department of Computer Science, Yonsei University.\n",
    "The data is expert-generated high-quality photoshopped face images. These images are not obtained from GAN.\n",
    "\n",
    "Method2:(This method did not work - requires more time)\n",
    "I have chosen to use CelebA data. \n",
    "Firstly, I wanted a data set that is publicly accessible to everyone.\n",
    "CelebA data contains 200,000+ images of celebrity faces and it includes diversity with people belonging to different races, genders,and ages. This data can be used as Real face Images.\n",
    "Fake Face images are then created using DCGAN(Deep Convolutional Generative Adversarial Network). \n",
    "I tried to generate fake face images using this method but it was very time consuming to train the model and generate a deep fake face image. \n",
    "Data set was downloaded from http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html website.\n",
    "I select random 400 images from that dataset to create celeb_face_real dataset.\n",
    "DCGA(Deep convolutional generative Adversarial network)was used to generate fake face images from the real face images. 80 epochs are run and the images generated after each epoch are stored in celeb_face_fake folder.\n",
    "\n",
    "\n",
    "b. How you went about collecting the data.\n",
    "Answer: I randomly selected around 1000 images from both fake and real images.\n",
    "For test dataset, seperate source is used and around 200 of real and fake images are collected.\n",
    "These images are placed under dataset folder.\n",
    "\n",
    "c. Besides fake/real labels, what other labels would you consider? Explain a simple method to sample a uniform dataset in the i.i.d sense, given the labels.\n",
    "Answer: Beside real/fake labels, I can consider the facial expression of a person(angry, sad, happy..) as labels. \n",
    "I can also try to recognize try to classify as a person with glasses or without glasses. \n",
    "There are many more.\n",
    "\n",
    "d. What API (e.g Pandas, etc.) you used to store and organize meta information about the dataset.\n",
    "Answer:YAML file is used to store information about the image dataset in a structured format.\n",
    "        It contains dataset name, creation date, author name, classes/categories, number of images, size and resolution of images, version of the meta data.\n",
    "\n",
    "e. Please share your mini-dataset as a zip file\n",
    "The zipfile is in the dataset folder. You can also download the dataset from https://drive.google.com/drive/folders/1Y8jfjAaej9GsIK1f3MUcjgiBH2qOywal?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "41e03936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_cnn_model():\n",
    "    model_cnn = keras.Sequential()\n",
    "    model_cnn.add(keras.layers.Conv2D(32,(3,3),activation='relu',input_shape=(224,224,3)))\n",
    "    model_cnn.add(keras.layers.MaxPool2D(2,2))\n",
    "    model_cnn.add(keras.layers.Conv2D(64,(3,3),activation='relu'))\n",
    "    model_cnn.add(keras.layers.MaxPool2D(2,2))\n",
    "\n",
    "    model_cnn.add(keras.layers.Conv2D(128,(3,3),activation='relu'))\n",
    "    model_cnn.add(keras.layers.MaxPool2D(2,2))\n",
    "\n",
    "    model_cnn.add(keras.layers.Conv2D(128,(3,3),activation='relu'))\n",
    "    model_cnn.add(keras.layers.MaxPool2D(2,2))\n",
    "\n",
    "    model_cnn.add(keras.layers.Flatten())\n",
    "\n",
    "    model_cnn.add(keras.layers.Dense(512,activation='relu'))\n",
    "\n",
    "    model_cnn.add(keras.layers.Dense(1,activation='sigmoid'))\n",
    "    model_cnn.compile(loss='binary_crossentropy',optimizer=Adam(0.0002), metrics=['acc'])\n",
    "    return model_cnn\n",
    " \n",
    "\n",
    "model_cnn = define_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist2 = model_cnn.fit(train,\n",
    "         steps_per_epoch = 25,\n",
    "         epochs = 30,\n",
    "         validation_data = val)\n",
    "\n",
    "model_cnn.save('final2_model.h5')\n",
    "_, acc = model_cnn.evaluate(val, steps=len(val), verbose=0)\n",
    "print('> %.3f' % (acc * 100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed774fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.title('Cross Entropy Loss')\n",
    "plt.plot(hist2.history['loss'], color='blue', label='train')\n",
    "plt.plot(hist2.history['val_loss'], color='orange', label='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f73d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(212)\n",
    "plt.title('Classification Accuracy')\n",
    "plt.plot(hist2.history['acc'], color='blue', label='train')\n",
    "plt.plot(hist2.history['val_acc'], color='orange', label='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae535af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = model_cnn.predict(real_test)\n",
    "y_test2 = real_test.classes\n",
    "#print(\"ROC-AUC Score:\", roc_auc_score(y_test2, y_pred2))\n",
    "threshold = 0.5\n",
    "pred = (y_pred2 > threshold).astype(int)\n",
    "\n",
    "accuracy = (pred == y_test2).mean()\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e7605a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c4ef0e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " conv1_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " conv1_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " pool1 (MaxPooling2D)        (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " conv2_1 (Conv2D)            (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " conv2_2 (Conv2D)            (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " pool2 (MaxPooling2D)        (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " conv3_1 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " conv3_2 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " conv3_3 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " pool3 (MaxPooling2D)        (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv4_1 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " conv4_2 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " conv4_3 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " pool4 (MaxPooling2D)        (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv5_1 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " conv5_2 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " conv5_3 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " pool5 (MaxPooling2D)        (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 2048)              51382272  \n",
      "                                                                 \n",
      " dense2 (Dense)              (None, 1)                 2049      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,099,009\n",
      "Trainable params: 66,099,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg_model = VGGFace(include_top=False, input_shape = (224,224,3))\n",
    "\n",
    "last_layer = vgg_model.get_layer('pool5').output\n",
    "flat_layer = Flatten(name='flatten')(last_layer)\n",
    "fc1 = Dense(2048, activation='relu', name='fc1')(flat_layer)\n",
    "dense2 = Dense(1, activation='sigmoid', name='dense2')(fc1)\n",
    "\n",
    "custom_vgg_model = Model(vgg_model.input, dense2)\n",
    "custom_vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2357f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_vgg_model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(0.0002), \n",
    "    metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f04687f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 4/25 [===>..........................] - ETA: 10:32 - loss: 0.7254 - acc: 0.5039"
     ]
    }
   ],
   "source": [
    "train_steps = 1632//64 #number of image/batch size\n",
    "valid_steps = 409//64\n",
    "\n",
    "history = custom_vgg_model.fit(\n",
    "    train,\n",
    "    epochs=50,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_data=val,\n",
    "    validation_steps=valid_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8355b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hist2 = custom_vgg_model.fit(train,\n",
    "#         steps_per_epoch = 25,\n",
    "#         epochs = 10,\n",
    "#         validation_data = val)\n",
    "\n",
    "#custom_vgg_model.save('final3_model.h5')\n",
    "#_, acc = custom_vgg_model.evaluate(val, steps=len(val), verbose=0)\n",
    "#print('> %.3f' % (acc * 100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4771c28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_vgg_model.save('vggface_v1.h5')\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b724c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = custom_vgg_model.predict(real_test)\n",
    "y_test = real_test.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc94ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
